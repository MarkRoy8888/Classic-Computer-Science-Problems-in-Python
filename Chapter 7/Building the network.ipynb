{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network will be built using classes of all three organisational units of the network: neurons, layers and the network itself.\n",
    "\n",
    "# Implementing neurons\n",
    "\n",
    "- We make all the variables of a __neuron__ mutable to be flexible.\n",
    "\n",
    "To go into neuron.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "from util import dot_product\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights: List[float],\n",
    "        learning_rate: float,\n",
    "        activation_function: Callable[[float], float],\n",
    "        derivative_activation_function: Callable[[float], float],\n",
    "    ) -> None:\n",
    "        self.weights: List[float] = weights\n",
    "        self.activation_function: Callable[[float], float] = activation_function\n",
    "        self.derivative_activation_function: Callable[\n",
    "            [float], float\n",
    "        ] = derivative_activation_function\n",
    "        self.learning_rate: float = learning_rate\n",
    "        # we don't know the output and delta yet so we initialise them to 0.0\n",
    "        self.output_cache: float = 0.0\n",
    "        self.delta: float = 0.0\n",
    "\n",
    "    def output(self, inputs: List[float]) -> float:\n",
    "        self.output_cache = dot_product(inputs, self.weights)\n",
    "        return self.activation_function(self.output_cache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Layers\n",
    "\n",
    "To go into layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Callable, Optional\n",
    "from random import random\n",
    "from neuron import Neuron\n",
    "from util import dot_product\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        previous_layer: Optional[Layer],\n",
    "        num_neurons: int,\n",
    "        learning_rate: float,\n",
    "        activation_function: Callable[[float], float],\n",
    "        derivative_activation_function: Callable[[float], float],\n",
    "    ) -> None:\n",
    "        self.previous_layer: Optional[Layer] = previous_layer\n",
    "        self.neurons: List[Neuron] = []\n",
    "        # the following could all be one large list comprehension\n",
    "        for i in range(num_neurons):\n",
    "            if previous_layer is None:\n",
    "                random_weights: List[float] = []\n",
    "            else:\n",
    "                # initialise one weight for each input, based on num_neurons\n",
    "                random_weights = [random() for _ in range(len(previous_layer.neurons))]\n",
    "            neuron: Neuron = Neuron(\n",
    "                random_weights,\n",
    "                learning_rate,\n",
    "                activation_function,\n",
    "                derivative_activation_function,\n",
    "            )\n",
    "            self.neurons.append(neuron)\n",
    "        self.output_cache: List[float] = [0.0 for _ in range(num_neurons)]\n",
    "\n",
    "    # this method process the inputs and return signal outputs\n",
    "    def outputs(self, inputs: List[float]) -> List[float]:\n",
    "        if self.previous_layer is None:\n",
    "            self.output_cache = inputs\n",
    "        else:\n",
    "            # get pass inputs through each neuron\n",
    "            self.output_cache = [n.output(inputs) for n in self.neurons]\n",
    "        return self.output_cache\n",
    "\n",
    "    # should only be called on output layer\n",
    "    def calculate_deltas_for_output_layer(self, expected: List[float]) -> None:\n",
    "        for n in range(len(self.neurons)):\n",
    "            # manipulate the delta inside the neuron\n",
    "            # derivative * output error\n",
    "            self.neurons[n].delta = self.neurons[n].derivative_activation_function(\n",
    "                self.neurons[n].output_cache\n",
    "            ) * (expected[n] - self.output_cache[n])\n",
    "\n",
    "    # should not be called on output layer\n",
    "    def calculate_deltas_for_hidden_layer(self, next_layer: Layer) -> None:\n",
    "        for index, neuron in enumerate(self.neurons):\n",
    "            # manipulate the delta inside the neuron\n",
    "            # derivative * output error\n",
    "            next_weights: List[float] = [n.weights[index] for n in next_layer.neurons]\n",
    "            next_deltas: List[float] = [n.delta for n in next_layer.neurons]\n",
    "            sum_weights_and_deltas: float = dot_product(next_weights, next_deltas)\n",
    "            neuron.delta = (\n",
    "                neuron.derivative_activation_function(neuron.output_cache)\n",
    "                * sum_weights_and_deltas\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Network\n",
    "\n",
    "\"The network itself has only one piece of state: the layers that it manages. The _Network_ class is responsible for initialising its constituent layers.\n",
    "\n",
    "The ____ init __ ()__ method takes a list of __int__  describing the structure of the network. e.g. [3, 4, 3] means the network has 3 neurons in its input layer, 4 neurons in its hidden layer, and 3 neurons in its output layer. In this simple neural network we assume all layers use the same activation function and have the same learning rate.\n",
    "\n",
    "To go in network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Callable, TypeVar, Tuple\n",
    "from functools import reduce\n",
    "from layer import Layer\n",
    "from util import sigmoid, derivatie_sigmoid\n",
    "\n",
    "T = TypeVar(\"T\")  # output type of interpretation of neural network\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_structure: List[int],\n",
    "        learning_rate: float,\n",
    "        activation_function: Callable[[float], float] = sigmoid,\n",
    "        derivative_activation_function: Callable[[float], float] = derivatie_sigmoid,\n",
    "    ) -> None:\n",
    "        if len(layer_structure) < 3:\n",
    "            raise ValueError(\n",
    "                \"Error: Should be at least 3 layers (1 input, \\\n",
    "                1 hidden, 1 output\"\n",
    "            )\n",
    "        # create a list to add layers\n",
    "        self.layers: List[Layer] = []\n",
    "        # input layer\n",
    "        # it has None previous layer, layer_structure[0] neurons etc\n",
    "        input_layer: Layer = Layer(\n",
    "            None,\n",
    "            layer_structure[0],\n",
    "            learning_rate,\n",
    "            activation_function,\n",
    "            derivative_activation_function,\n",
    "        )\n",
    "        self.layers.append(input_layer)\n",
    "\n",
    "        # hidden layers and output layer\n",
    "        # we use enumerate to get the index of the previous layer\n",
    "        for previous, num_neurons in enumerate(layer_structure[1::]):\n",
    "            next_layer = Layer(\n",
    "                self.layers[previous],\n",
    "                num_neurons,\n",
    "                learning_rate,\n",
    "                activation_function,\n",
    "                derivative_activation_function,\n",
    "            )\n",
    "            self.layers.append(next_layer)\n",
    "\n",
    "    # We use reduce() in output() to pass signals from one layer to the next repeatedly\n",
    "    # through the whole network\n",
    "    def outputs(self, input: List[float]) -> List[float]:\n",
    "        return reduce(lambda inputs, layer: layer.outputs(inputs), self.layers, input)\n",
    "\n",
    "    # The backpropagate() method is responsible for computing deltas for every neuron.\n",
    "    # Figure out each neuron's changes based on the errors of the output\n",
    "    # versus the expected outcome\n",
    "    def backpropagate(self, expected: List[float]) -> None:\n",
    "        # calculate delta for output layer neurons\n",
    "        last_layer: int = len(self.layers) - 1\n",
    "        # The .calculate_deltas_for_output_layer method takes the expected\n",
    "        # (final) output as input\n",
    "        self.layers[last_layer].calculate_deltas_for_output_layer(expected)\n",
    "        # calculated delta for hiddern layers in reverse order\n",
    "        for l in range(last_layer - 1, 0, -1):\n",
    "            # The .calculate_deltas_for_hidden_layer method takes the next\n",
    "            # layer as input\n",
    "            self.layers[l].calculate_deltas_for_hidden_layer(self.layers[l + 1])\n",
    "\n",
    "    # backpropagate() doesn't actually change any weights\n",
    "    # this function uses the deltas calculated in backpropagate() to\n",
    "    # actually make changes to the weights\n",
    "    def update_weights(self) -> None:\n",
    "        for layer in self.layers[1:]:  # skip input layer\n",
    "            for neuron in layer.neurons:\n",
    "                for w in range(len(neuron.weights)):\n",
    "                    neuron.weights[w] = neuron.weights[w] + (\n",
    "                        neuron.learning_rate\n",
    "                        * (layer.previous_layer.output_cache[w])\n",
    "                        * neuron.delta\n",
    "                    )\n",
    "\n",
    "    # train() uses the results of outputs() run over many inputs and compared\n",
    "    # against expected outputs to feed backpropagate() and update_weights()\n",
    "    def train(self, inputs: List[List[float]], expecteds: List[List[float]]) -> None:\n",
    "        for location, xs in enumerate(inputs):\n",
    "            # expected output list\n",
    "            ys: List[float] = expecteds[location]\n",
    "            # calculate expected output\n",
    "            outs: List[float] = self.outputs(xs)  # unused variable?\n",
    "            self.backpropagate(ys)\n",
    "            self.update_weights()\n",
    "\n",
    "    # for generalised results that require classification\n",
    "    # this function will return the correct number of trials\n",
    "    # and the percentage correct number of trials\n",
    "    # interpret_output() must take the floating-point numbers it gets as\n",
    "    # output from the network and convert them into something comparable to\n",
    "    # the expected outputs. It is a custom function specific to a data set.\n",
    "    def validate(\n",
    "        self,\n",
    "        inputs: List[[List[float]]],\n",
    "        expecteds: List[T],\n",
    "        interpret_output: Callable[[List[float]], T],\n",
    "    ) -> Tuple[int, int, float]:\n",
    "        # count of corrects, initialised to 0\n",
    "        correct: int = 0\n",
    "        for input, expected in zip(inputs, expecteds):\n",
    "            result: T = interpret_output(self.outputs(input))\n",
    "            if result == expected:\n",
    "                correct += 1\n",
    "        percentage: float = correct / len(inputs)\n",
    "        return correct, len(inputs), percentage\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
